{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Baixar recursos do NLTK\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Função para extrair texto de PDFs\n",
    "def pdf_para_txt(caminho_pdf):\n",
    "    with open(caminho_pdf, \"rb\") as f:\n",
    "        leitor = PyPDF2.PdfReader(f)\n",
    "        texto = \"\"\n",
    "        for pagina in range(len(leitor.pages)):\n",
    "            texto += leitor.pages[pagina].extract_text()\n",
    "    return texto\n",
    "\n",
    "# Diretórios com os PDFs\n",
    "diretorios = {\n",
    "    'poesia': 'pdfs/poesia/',\n",
    "    'prosa': 'pdfs/prosa/',\n",
    "    'jornalismo': 'pdfs/jornalismo/'\n",
    "}\n",
    "\n",
    "# Função para limpar e remover stopwords\n",
    "def limpar_texto(texto):\n",
    "    texto = texto.replace(\"\\n\", \" \")\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    palavras = nltk.word_tokenize(texto.lower())\n",
    "    palavras_limpa = [palavra for palavra in palavras if palavra.isalnum() and palavra not in stop_words]\n",
    "    return \" \".join(palavras_limpa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Extraindo textos e gerando classes\n",
    "textos = []\n",
    "classes = []\n",
    "\n",
    "for classe, caminho in diretorios.items():\n",
    "    for arquivo in os.listdir(caminho):\n",
    "        if arquivo.endswith(\".pdf\"):\n",
    "            texto = pdf_para_txt(os.path.join(caminho, arquivo))\n",
    "            texto_limpo = limpar_texto(texto)\n",
    "            textos.append(texto_limpo)\n",
    "            classes.append(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz BoW:  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Vocabulário:  ['01' '02' '03' ... '⁷sir' '⁸itisreservedtotheendoftheday' '⁹it']\n",
      "Classes:  ['poesia' 'poesia' 'poesia' 'poesia' 'poesia' 'poesia' 'poesia' 'poesia'\n",
      " 'poesia' 'poesia' 'poesia' 'prosa' 'prosa' 'prosa' 'prosa' 'prosa'\n",
      " 'prosa' 'prosa' 'prosa' 'prosa' 'prosa' 'prosa' 'jornalismo' 'jornalismo'\n",
      " 'jornalismo' 'jornalismo' 'jornalismo' 'jornalismo' 'jornalismo'\n",
      " 'jornalismo' 'jornalismo' 'jornalismo' 'jornalismo']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Criando a matriz Bag of Words\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(textos)\n",
    "\n",
    "print(\"Matriz BoW: \", X.toarray())\n",
    "print(\"Vocabulário: \", vectorizer.get_feature_names_out())\n",
    "print(\"Classes: \", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados por Fold:\n",
      "Fold 1: Acurácia = 0.75, F1-Score = 0.65\n",
      "Fold 2: Acurácia = 0.75, F1-Score = 0.65\n",
      "Fold 3: Acurácia = 0.75, F1-Score = 0.67\n",
      "Fold 4: Acurácia = 1.00, F1-Score = 1.00\n",
      "Fold 5: Acurácia = 0.67, F1-Score = 0.56\n",
      "Fold 6: Acurácia = 1.00, F1-Score = 1.00\n",
      "Fold 7: Acurácia = 0.67, F1-Score = 0.56\n",
      "Fold 8: Acurácia = 0.67, F1-Score = 0.56\n",
      "Fold 9: Acurácia = 1.00, F1-Score = 1.00\n",
      "Fold 10: Acurácia = 0.33, F1-Score = 0.33\n",
      "\n",
      "Resultados Gerais:\n",
      "Acurácia Média: 0.76, Desvio Padrão: 0.20\n",
      "F1-Score Média: 0.70, Desvio Padrão: 0.22\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Criando o modelo de árvore de decisão\n",
    "modelo = DecisionTreeClassifier()\n",
    "\n",
    "# Configurando a validação cruzada estratificada\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# Listas para armazenar as métricas\n",
    "acuracias = []\n",
    "f1_scores = []\n",
    "\n",
    "# Validação cruzada\n",
    "for train_index, test_index in kf.split(X, classes):\n",
    "    # Dividindo os dados em treino e teste para a rodada atual\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = [classes[i] for i in train_index], [classes[i] for i in test_index]\n",
    "    \n",
    "    # Treinando o modelo\n",
    "    modelo.fit(X_train, y_train)\n",
    "    \n",
    "    # Fazendo previsões\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    \n",
    "    # Calculando métricas\n",
    "    acuracia = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Armazenando os resultados\n",
    "    acuracias.append(acuracia)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calcular média e desvio padrão das métricas\n",
    "acuracia_media, acuracia_desvio = np.mean(acuracias), np.std(acuracias)\n",
    "f1_media, f1_desvio = np.mean(f1_scores), np.std(f1_scores)\n",
    "\n",
    "# Exibir resultados\n",
    "print(\"Resultados por Fold:\")\n",
    "for i, (acc, f1) in enumerate(zip(acuracias, f1_scores), 1):\n",
    "    print(f\"Fold {i}: Acurácia = {acc:.2f}, F1-Score = {f1:.2f}\")\n",
    "\n",
    "print(\"\\nResultados Gerais:\")\n",
    "print(f\"Acurácia Média: {acuracia_media:.2f}, Desvio Padrão: {acuracia_desvio:.2f}\")\n",
    "print(f\"F1-Score Média: {f1_media:.2f}, Desvio Padrão: {f1_desvio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "Melhores hiperparâmetros: {'criterion': 'entropy', 'splitter': 'best'}\n",
      "Melhor acurácia: 0.80\n",
      "Configuração de Hiperparâmetros 1: {'criterion': 'gini', 'splitter': 'best'}\n",
      "  Fold 1: Acurácia = 0.75, F1-score = 0.65\n",
      "  Fold 2: Acurácia = 0.50, F1-score = 0.38\n",
      "  Fold 3: Acurácia = 0.75, F1-score = 0.67\n",
      "  Fold 4: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 5: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 6: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 7: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 8: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 9: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 10: Acurácia = 0.67, F1-score = 0.56\n",
      "  Acurácia média: 0.77, Desvio padrão: 0.17\n",
      "  F1-score médio: 0.69, Desvio padrão: 0.21\n",
      "--------------------------------------------------\n",
      "Configuração de Hiperparâmetros 2: {'criterion': 'gini', 'splitter': 'random'}\n",
      "  Fold 1: Acurácia = 0.50, F1-score = 0.50\n",
      "  Fold 2: Acurácia = 0.50, F1-score = 0.50\n",
      "  Fold 3: Acurácia = 0.75, F1-score = 0.67\n",
      "  Fold 4: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 5: Acurácia = 0.33, F1-score = 0.33\n",
      "  Fold 6: Acurácia = 0.33, F1-score = 0.33\n",
      "  Fold 7: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 8: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 9: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 10: Acurácia = 1.00, F1-score = 1.00\n",
      "  Acurácia média: 0.71, Desvio padrão: 0.27\n",
      "  F1-score médio: 0.69, Desvio padrão: 0.27\n",
      "--------------------------------------------------\n",
      "Configuração de Hiperparâmetros 3: {'criterion': 'entropy', 'splitter': 'best'}\n",
      "  Fold 1: Acurácia = 0.75, F1-score = 0.65\n",
      "  Fold 2: Acurácia = 0.50, F1-score = 0.38\n",
      "  Fold 3: Acurácia = 0.75, F1-score = 0.67\n",
      "  Fold 4: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 5: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 6: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 7: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 8: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 9: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 10: Acurácia = 0.67, F1-score = 0.56\n",
      "  Acurácia média: 0.80, Desvio padrão: 0.18\n",
      "  F1-score médio: 0.74, Desvio padrão: 0.23\n",
      "--------------------------------------------------\n",
      "Configuração de Hiperparâmetros 4: {'criterion': 'entropy', 'splitter': 'random'}\n",
      "  Fold 1: Acurácia = 0.50, F1-score = 0.50\n",
      "  Fold 2: Acurácia = 0.75, F1-score = 0.65\n",
      "  Fold 3: Acurácia = 0.75, F1-score = 0.67\n",
      "  Fold 4: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 5: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 6: Acurácia = 0.33, F1-score = 0.22\n",
      "  Fold 7: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 8: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 9: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 10: Acurácia = 1.00, F1-score = 1.00\n",
      "  Acurácia média: 0.70, Desvio padrão: 0.19\n",
      "  F1-score médio: 0.63, Desvio padrão: 0.22\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "\n",
    "# Criando o modelo base\n",
    "modelo = DecisionTreeClassifier()\n",
    "\n",
    "# Configuração de hiperparâmetros para teste\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random']\n",
    "}\n",
    "\n",
    "# Métricas para otimização\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted'),\n",
    "}\n",
    "\n",
    "# Configurando o GridSearchCV com validação cruzada\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=modelo,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit='accuracy',\n",
    "    cv=10,  # Validação cruzada estratificada com 10 folds\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Treinando o modelo com GridSearch\n",
    "grid_search.fit(X, classes)\n",
    "\n",
    "# Exibindo os melhores hiperparâmetros e resultados\n",
    "print(\"Melhores hiperparâmetros:\", grid_search.best_params_)\n",
    "print(f\"Melhor acurácia: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Obtendo os resultados detalhados\n",
    "resultados = grid_search.cv_results_\n",
    "\n",
    "# Iterar sobre cada combinação de hiperparâmetros\n",
    "for i in range(len(resultados['params'])):\n",
    "    print(f\"Configuração de Hiperparâmetros {i+1}: {resultados['params'][i]}\")\n",
    "\n",
    "    # Métricas para cada fold\n",
    "    for fold in range(10):  # 10 folds\n",
    "        acuracia_fold = resultados[f'split{fold}_test_accuracy'][i]\n",
    "        f1_fold = resultados[f'split{fold}_test_f1_weighted'][i]\n",
    "        print(f\"  Fold {fold+1}: Acurácia = {acuracia_fold:.2f}, F1-score = {f1_fold:.2f}\")\n",
    "\n",
    "    # Métricas médias\n",
    "    acuracia_media = resultados['mean_test_accuracy'][i]\n",
    "    f1_media = resultados['mean_test_f1_weighted'][i]\n",
    "    acuracia_std = resultados['std_test_accuracy'][i]\n",
    "    f1_std = resultados['std_test_f1_weighted'][i]\n",
    "\n",
    "    print(f\"  Acurácia média: {acuracia_media:.2f}, Desvio padrão: {acuracia_std:.2f}\")\n",
    "    print(f\"  F1-score médio: {f1_media:.2f}, Desvio padrão: {f1_std:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Criando o modelo base\n",
    "modelo = KNeighborsClassifier()\n",
    "\n",
    "# Configuração de hiperparâmetros para teste\n",
    "param_grid = {\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree']\n",
    "}\n",
    "\n",
    "# Métricas para otimização\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted'),\n",
    "}\n",
    "\n",
    "# Configurando o GridSearchCV com validação cruzada\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=modelo,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit='accuracy',\n",
    "    cv=10,  # Validação cruzada estratificada com 10 folds\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Treinando o modelo com GridSearch\n",
    "grid_search.fit(X, classes)\n",
    "\n",
    "# Exibindo os melhores hiperparâmetros e resultados\n",
    "print(\"Melhores hiperparâmetros:\", grid_search.best_params_)\n",
    "print(f\"Melhor acurácia: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Obtendo os resultados detalhados\n",
    "resultados = grid_search.cv_results_\n",
    "\n",
    "# Iterar sobre cada combinação de hiperparâmetros\n",
    "for i in range(len(resultados['params'])):\n",
    "    print(f\"Configuração de Hiperparâmetros {i+1}: {resultados['params'][i]}\")\n",
    "\n",
    "    # Métricas para cada fold\n",
    "    for fold in range(10):  # 10 folds\n",
    "        acuracia_fold = resultados[f'split{fold}_test_accuracy'][i]\n",
    "        f1_fold = resultados[f'split{fold}_test_f1_weighted'][i]\n",
    "        print(f\"  Fold {fold+1}: Acurácia = {acuracia_fold:.2f}, F1-score = {f1_fold:.2f}\")\n",
    "\n",
    "    # Métricas médias\n",
    "    acuracia_media = resultados['mean_test_accuracy'][i]\n",
    "    f1_media = resultados['mean_test_f1_weighted'][i]\n",
    "    acuracia_std = resultados['std_test_accuracy'][i]\n",
    "    f1_std = resultados['std_test_f1_weighted'][i]\n",
    "\n",
    "    print(f\"  Acurácia média: {acuracia_media:.2f}, Desvio padrão: {acuracia_std:.2f}\")\n",
    "    print(f\"  F1-score médio: {f1_media:.2f}, Desvio padrão: {f1_std:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Criando o modelo base\n",
    "modelo = MultinomialNB()\n",
    "\n",
    "# Configuração de hiperparâmetros para teste\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1.0],\n",
    "    'force_alpha': [True, False]\n",
    "}\n",
    "\n",
    "# Métricas para otimização\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted'),\n",
    "}\n",
    "\n",
    "# Configurando o GridSearchCV com validação cruzada\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=modelo,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit='accuracy',\n",
    "    cv=10,  # Validação cruzada estratificada com 10 folds\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Treinando o modelo com GridSearch\n",
    "grid_search.fit(X, classes)\n",
    "\n",
    "# Exibindo os melhores hiperparâmetros e resultados\n",
    "print(\"Melhores hiperparâmetros:\", grid_search.best_params_)\n",
    "print(f\"Melhor acurácia: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Obtendo os resultados detalhados\n",
    "resultados = grid_search.cv_results_\n",
    "\n",
    "# Iterar sobre cada combinação de hiperparâmetros\n",
    "for i in range(len(resultados['params'])):\n",
    "    print(f\"Configuração de Hiperparâmetros {i+1}: {resultados['params'][i]}\")\n",
    "\n",
    "    # Métricas para cada fold\n",
    "    for fold in range(10):  # 10 folds\n",
    "        acuracia_fold = resultados[f'split{fold}_test_accuracy'][i]\n",
    "        f1_fold = resultados[f'split{fold}_test_f1_weighted'][i]\n",
    "        print(f\"  Fold {fold+1}: Acurácia = {acuracia_fold:.2f}, F1-score = {f1_fold:.2f}\")\n",
    "\n",
    "    # Métricas médias\n",
    "    acuracia_media = resultados['mean_test_accuracy'][i]\n",
    "    f1_media = resultados['mean_test_f1_weighted'][i]\n",
    "    acuracia_std = resultados['std_test_accuracy'][i]\n",
    "    f1_std = resultados['std_test_f1_weighted'][i]\n",
    "\n",
    "    print(f\"  Acurácia média: {acuracia_media:.2f}, Desvio padrão: {acuracia_std:.2f}\")\n",
    "    print(f\"  F1-score médio: {f1_media:.2f}, Desvio padrão: {f1_std:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Criando o modelo base\n",
    "modelo = LogisticRegression()\n",
    "\n",
    "# Configuração de hiperparâmetros para teste\n",
    "param_grid = {\n",
    "    'solver': ['lbfgs', 'saga'],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "# Métricas para otimização\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted'),\n",
    "}\n",
    "\n",
    "# Configurando o GridSearchCV com validação cruzada\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=modelo,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit='accuracy',\n",
    "    cv=10,  # Validação cruzada estratificada com 10 folds\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Treinando o modelo com GridSearch\n",
    "grid_search.fit(X, classes)\n",
    "\n",
    "# Exibindo os melhores hiperparâmetros e resultados\n",
    "print(\"Melhores hiperparâmetros:\", grid_search.best_params_)\n",
    "print(f\"Melhor acurácia: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Obtendo os resultados detalhados\n",
    "resultados = grid_search.cv_results_\n",
    "\n",
    "# Iterar sobre cada combinação de hiperparâmetros\n",
    "for i in range(len(resultados['params'])):\n",
    "    print(f\"Configuração de Hiperparâmetros {i+1}: {resultados['params'][i]}\")\n",
    "\n",
    "    # Métricas para cada fold\n",
    "    for fold in range(10):  # 10 folds\n",
    "        acuracia_fold = resultados[f'split{fold}_test_accuracy'][i]\n",
    "        f1_fold = resultados[f'split{fold}_test_f1_weighted'][i]\n",
    "        print(f\"  Fold {fold+1}: Acurácia = {acuracia_fold:.2f}, F1-score = {f1_fold:.2f}\")\n",
    "\n",
    "    # Métricas médias\n",
    "    acuracia_media = resultados['mean_test_accuracy'][i]\n",
    "    f1_media = resultados['mean_test_f1_weighted'][i]\n",
    "    acuracia_std = resultados['std_test_accuracy'][i]\n",
    "    f1_std = resultados['std_test_f1_weighted'][i]\n",
    "\n",
    "    print(f\"  Acurácia média: {acuracia_media:.2f}, Desvio padrão: {acuracia_std:.2f}\")\n",
    "    print(f\"  F1-score médio: {f1_media:.2f}, Desvio padrão: {f1_std:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> ()\n",
      "<class 'numpy.ndarray'> 33\n"
     ]
    }
   ],
   "source": [
    "print(type(X), X.shape)\n",
    "print(type(classes), len(classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "Melhores hiperparâmetros: {'activation': 'tanh', 'hidden_layer_sizes': (100,)}\n",
      "Melhor acurácia: 0.97\n",
      "Configuração de Hiperparâmetros 1: {'activation': 'relu', 'hidden_layer_sizes': (50,)}\n",
      "  Fold 1: Acurácia = 0.75, F1-score = 0.75\n",
      "  Fold 2: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 3: Acurácia = 0.75, F1-score = 0.67\n",
      "  Fold 4: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 5: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 6: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 7: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 8: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 9: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 10: Acurácia = 0.33, F1-score = 0.17\n",
      "  Acurácia média: 0.82, Desvio padrão: 0.21\n",
      "  F1-score médio: 0.77, Desvio padrão: 0.27\n",
      "--------------------------------------------------\n",
      "Configuração de Hiperparâmetros 2: {'activation': 'relu', 'hidden_layer_sizes': (100,)}\n",
      "  Fold 1: Acurácia = 0.75, F1-score = 0.75\n",
      "  Fold 2: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 3: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 4: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 5: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 6: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 7: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 8: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 9: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 10: Acurácia = 1.00, F1-score = 1.00\n",
      "  Acurácia média: 0.94, Desvio padrão: 0.12\n",
      "  F1-score médio: 0.93, Desvio padrão: 0.15\n",
      "--------------------------------------------------\n",
      "Configuração de Hiperparâmetros 3: {'activation': 'tanh', 'hidden_layer_sizes': (50,)}\n",
      "  Fold 1: Acurácia = 0.75, F1-score = 0.75\n",
      "  Fold 2: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 3: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 4: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 5: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 6: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 7: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 8: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 9: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 10: Acurácia = 1.00, F1-score = 1.00\n",
      "  Acurácia média: 0.94, Desvio padrão: 0.12\n",
      "  F1-score médio: 0.93, Desvio padrão: 0.15\n",
      "--------------------------------------------------\n",
      "Configuração de Hiperparâmetros 4: {'activation': 'tanh', 'hidden_layer_sizes': (100,)}\n",
      "  Fold 1: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 2: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 3: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 4: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 5: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 6: Acurácia = 0.67, F1-score = 0.56\n",
      "  Fold 7: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 8: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 9: Acurácia = 1.00, F1-score = 1.00\n",
      "  Fold 10: Acurácia = 1.00, F1-score = 1.00\n",
      "  Acurácia média: 0.97, Desvio padrão: 0.10\n",
      "  F1-score médio: 0.96, Desvio padrão: 0.13\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Arquivos\\EngSoft\\IA\\trab\\bow\\.env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Criando o modelo base\n",
    "modelo = MLPClassifier(max_iter=10)\n",
    "\n",
    "# Configuração de hiperparâmetros para teste\n",
    "param_grid = {\n",
    "    'max_iter': [10, 200],\n",
    "    'activation': ['relu', 'tanh']\n",
    "}\n",
    "\n",
    "# Métricas para otimização\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_weighted': make_scorer(f1_score, average='weighted'),\n",
    "}\n",
    "\n",
    "# Configurando o GridSearchCV com validação cruzada\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=modelo,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit='accuracy',\n",
    "    cv=10,  # Validação cruzada estratificada com 10 folds\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Treinando o modelo com GridSearch\n",
    "grid_search.fit(X, classes)\n",
    "\n",
    "# Exibindo os melhores hiperparâmetros e resultados\n",
    "print(\"Melhores hiperparâmetros:\", grid_search.best_params_)\n",
    "print(f\"Melhor acurácia: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Obtendo os resultados detalhados\n",
    "resultados = grid_search.cv_results_\n",
    "\n",
    "# Iterar sobre cada combinação de hiperparâmetros\n",
    "for i in range(len(resultados['params'])):\n",
    "    print(f\"Configuração de Hiperparâmetros {i+1}: {resultados['params'][i]}\")\n",
    "\n",
    "    # Métricas para cada fold\n",
    "    for fold in range(10):  # 10 folds\n",
    "        acuracia_fold = resultados[f'split{fold}_test_accuracy'][i]\n",
    "        f1_fold = resultados[f'split{fold}_test_f1_weighted'][i]\n",
    "        print(f\"  Fold {fold+1}: Acurácia = {acuracia_fold:.2f}, F1-score = {f1_fold:.2f}\")\n",
    "\n",
    "    # Métricas médias\n",
    "    acuracia_media = resultados['mean_test_accuracy'][i]\n",
    "    f1_media = resultados['mean_test_f1_weighted'][i]\n",
    "    acuracia_std = resultados['std_test_accuracy'][i]\n",
    "    f1_std = resultados['std_test_f1_weighted'][i]\n",
    "\n",
    "    print(f\"  Acurácia média: {acuracia_media:.2f}, Desvio padrão: {acuracia_std:.2f}\")\n",
    "    print(f\"  F1-score médio: {f1_media:.2f}, Desvio padrão: {f1_std:.2f}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
